I am process 104000, running on gpu22: starting (Sat Mar  5 20:54:48 2022)
args.batch_size 8
args Namespace(a_imagenet_pretrain=True, a_patch_num=256, audioset_pretrain=False, bal=None, batch_size=8, data_eval='test_iemocap_dataset.json', data_train='train_iemocap_dataset.json', data_val='valid_iemocap_dataset.json', dataset='iemocap', early_stop=5, exp_dir='model_dir/temp_imagenet_8_0.0001_40_fusion', face_size=128, freqm=0, fstride=16, k=-1, label_csv='', lr=0.0001, lr_patience=2, mixup=0, modal='tav', model='ast_model_video_deit', n_class=6, n_epochs=40, n_print_steps=100, num_height=384, num_width=384, num_workers=12, optim='adam', save_model=None, scale=0, seed=1235, text_lr_factor=100, text_max_len=300, time_dim_split=True, timem=0, tstride=16, v_imagenet_pretrain=True, v_patch_num=256)
now process iemocap
---------------the train dataloader---------------
number of classes is 6
now process iemocap
---------------the evaluation dataloader---------------
number of classes is 6
---------------AST Model Summary---------------
ImageNet pretraining: True, AudioSet pretraining: False
frequncey stride=16, time stride=16
number of patches=512
---------------AST Model Summary---------------
ImageNet pretraining: True, AudioSet pretraining: False
frequncey stride=16, time stride=16
number of patches=256
Some weights of the model checkpoint at /users10/zyzhang/graduationProject/data/pretrain_model/bert_base_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Creating experiment directory: model_dir/temp_imagenet_8_0.0001_40_fusion
Now starting training for 40 epochs
running on cuda
Total parameter number is : 305.026 million
Total trainable parameter number is : 305.026 million
scheduler for iemocap is used
now training with iemocap, main metrics: acc, loss function: BCEWithLogitsLoss(), learning rate scheduler: <torch.optim.lr_scheduler.MultiStepLR object at 0x7f8c5fb3f050>
current #steps=0, #epochs=1
start training...
---------------
2022-03-05 20:55:25.721691
current #epochs=1, #steps=0
Traceback (most recent call last):
  File "run.py", line 277, in <module>
    train(mt_model, train_loader, val_loader, args, tokenizer_model)
  File "/users10/zyzhang/multimodel/E2EMMT_interact/traintest.py", line 159, in train
    scaler.scale(loss).backward()
  File "/users10/zyzhang/anaconda3/envs/SparseEnd2End/lib/python3.7/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/users10/zyzhang/anaconda3/envs/SparseEnd2End/lib/python3.7/site-packages/torch/autograd/__init__.py", line 149, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 146.00 MiB (GPU 0; 15.78 GiB total capacity; 13.91 GiB already allocated; 39.75 MiB free; 14.29 GiB reserved in total by PyTorch)