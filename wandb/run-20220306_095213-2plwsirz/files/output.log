I am process 125510, running on gpu24: starting (Sun Mar  6 09:52:20 2022)
args.batch_size 8
args Namespace(a_imagenet_pretrain=True, a_patch_num=256, audioset_pretrain=False, bal=None, batch_size=8, data_eval='test_iemocap_dataset.json', data_train='train_iemocap_dataset.json', data_val='valid_iemocap_dataset.json', dataset='iemocap', early_stop=5, exp_dir='model_dir/compare_loss_ast_model_video_deit_iemocap', face_size=128, freqm=0, fstride=16, k=-1, label_csv='', lr=0.0001, lr_patience=2, mixup=0, modal='tav', model='ast_model_video_deit', n_class=6, n_epochs=40, n_print_steps=100, num_height=384, num_width=384, num_workers=12, optim='adam', save_model=None, scale=0, seed=1235, text_lr_factor=100, text_max_len=300, time_dim_split=True, timem=0, tstride=16, v_imagenet_pretrain=True, v_patch_num=256)
now process iemocap
---------------the train dataloader---------------
number of classes is 6
now process iemocap
---------------the evaluation dataloader---------------
number of classes is 6
---------------AST Model Summary---------------
ImageNet pretraining: True, AudioSet pretraining: False
frequncey stride=16, time stride=16
number of patches=512
---------------AST Model Summary---------------
ImageNet pretraining: True, AudioSet pretraining: False
frequncey stride=16, time stride=16
number of patches=256
Some weights of the model checkpoint at /users10/zyzhang/graduationProject/data/pretrain_model/bert_base_uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Creating experiment directory: model_dir/compare_loss_ast_model_video_deit_iemocap
Now starting training for 40 epochs
running on cuda
Total parameter number is : 305.026 million
Total trainable parameter number is : 305.026 million
scheduler for iemocap is used
now training with iemocap, main metrics: acc, loss function: BCEWithLogitsLoss(), learning rate scheduler: <torch.optim.lr_scheduler.MultiStepLR object at 0x7f46c9ec2150>
current #steps=0, #epochs=1
start training...
---------------
2022-03-06 09:52:54.428542
current #epochs=1, #steps=0
Epoch: [1][100/646]	Per Sample Total Time 0.09061	Per Sample Data Time 0.01846	Per Sample DNN Time 0.07214	Train Loss 0.4693	
Epoch: [1][200/646]	Per Sample Total Time 0.08130	Per Sample Data Time 0.00967	Per Sample DNN Time 0.07163	Train Loss 0.4364	
Epoch: [1][300/646]	Per Sample Total Time 0.07794	Per Sample Data Time 0.00670	Per Sample DNN Time 0.07125	Train Loss 0.4175	
Epoch: [1][400/646]	Per Sample Total Time 0.07908	Per Sample Data Time 0.00794	Per Sample DNN Time 0.07114	Train Loss 0.4044	
Epoch: [1][500/646]	Per Sample Total Time 0.07747	Per Sample Data Time 0.00658	Per Sample DNN Time 0.07089	Train Loss 0.3931	
Traceback (most recent call last):
  File "run.py", line 277, in <module>
    train(mt_model, train_loader, val_loader, args, tokenizer_model)
  File "/users10/zyzhang/multimodel/E2EMMT_interact/traintest.py", line 159, in train
    scaler.scale(loss).backward()
  File "/users10/zyzhang/anaconda3/envs/SparseEnd2End/lib/python3.7/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/users10/zyzhang/anaconda3/envs/SparseEnd2End/lib/python3.7/site-packages/torch/autograd/__init__.py", line 149, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt